
# Ubundi Codex Agent

Ubundi Codex Agent is an AI-powered assistant for processing, analyzing, and interacting with resume and cover letter data. It leverages advanced language models, FAISS vector databases, and fine-tuning techniques for customized performance.

---

## ğŸš€ Features

- ğŸ“„ **Resume & Cover Letter Processing** â€“ Ingests and analyzes documents in multiple formats.  
- ğŸ” **Vector Database Integration** â€“ Uses FAISS for efficient similarity search.  
- ğŸ§  **Fine-tuning & Custom Models** â€“ Supports LoRA fine-tuning for GPT-2 and model adapters.  
- ğŸŒ **API & UI** â€“ REST API endpoints and a simple user interface.  
- ğŸ›  **Extensible Utilities** â€“ Modular data loaders and web tools.  

---

## ğŸ“‚ Project Structure


.
â”œâ”€â”€ .env
â”œâ”€â”€ persona.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ resume\_instructions.jsonl
â”‚   â””â”€â”€ info/
â”‚       â”œâ”€â”€ Clarence\_Ebebe\_Cover\_Letter.pdf
â”‚       â”œâ”€â”€ ClarenceEbebeResume\_AIEngineer\_Resume.pdf
â”‚       â”œâ”€â”€ cover.txt
â”‚       â”œâ”€â”€ extra.md
â”‚       â”œâ”€â”€ extra.txt
â”‚       â””â”€â”€ resume.txt
â”œâ”€â”€ models/
â”‚   â””â”€â”€ lora-resume-gpt2/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ build\_vector\_db.py
â”‚   â”œâ”€â”€ finetune.py
â”‚   â”œâ”€â”€ ingest\_new\.py
â”‚   â”œâ”€â”€ prepare\_data.py
â”‚   â””â”€â”€ ui.py
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ faiss/
â””â”€â”€ utils/
â”œâ”€â”€ loader.py
â””â”€â”€ webtools.py



---

## âš™ï¸ Setup

### 1. Clone the Repository
```sh
git clone <repo-url>
cd Ubundi_Codex_Agent


### 2. Install Dependencies

```sh
pip install -r requirements.txt
```

### 3. Configure Environment Variables

```sh
cp .env.example .env
# Update the .env file with your configs
```

### 4. Prepare Data

Place your resume and cover letter files inside:

```
data/info/
```

---

## ğŸ›  Usage

### Build the Vector Database

```sh
python scripts/build_vector_db.py
```

### Fine-tune the Model (Optional)

```sh
python scripts/finetune.py \
    --data_path data/resume_instructions.jsonl \
    --output_dir models/lora-resume-gpt2 \
    --model_name gpt2 \
    --epochs 3 \
    --batch_size 4
```

> Tweak `epochs` and `batch_size` for your needs.

### Run the API

```sh
python scripts/api.py
```

### Run the UI

```sh
python scripts/ui.py
```

---

## ğŸ“œ Scripts Overview

* `agent.py` â€“ Core agent logic
* `api.py` â€“ REST API server
* `build_vector_db.py` â€“ Builds FAISS vector database
* `finetune.py` â€“ Fine-tunes the language model
* `ingest_new.py` â€“ Adds new data to the system
* `prepare_data.py` â€“ Cleans/prepares data for processing
* `ui.py` â€“ Launches the user interface

---

## ğŸ”§ Utilities

* `loader.py` â€“ Data loading utilities
* `webtools.py` â€“ Web tools and helpers

---

## ğŸ’¾ Storage

* `storage/faiss/` â€“ FAISS index files

---

## ğŸ¤– Models

* `lora-resume-gpt2/` â€“ Fine-tuned GPT-2 model with LoRA adapters

---


